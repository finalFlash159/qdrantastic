# ğŸ›©ï¸ Vector Database

## 1. Sparse vectors ğŸ—¿

A sparse vector is a special representation of high-dimensional vectors where most elements are zero, and only a few dimensions have non-zero values. 
(VÃ­ dá»¥, náº¿u tá»« Ä‘iá»ƒn cÃ³ 50,000 tá»«, má»™t vÄƒn báº£n chá»‰ chá»©a 100 tá»« sáº½ cÃ³ vector vá»›i 49,900 giÃ¡ trá»‹ 0 vÃ  100 giÃ¡ trá»‹ khÃ¡c 0.)
  * ChÃ­nh xÃ¡c trong khá»›p tá»«: GiÃºp náº¯m báº¯t chÃ­nh xÃ¡c cÃ¡c tá»« khÃ³a xuáº¥t hiá»‡n trong vÄƒn báº£n, há»¯u Ã­ch cho cÃ¡c tÃ¡c vá»¥ truy xuáº¥t dá»±a trÃªn tá»« khÃ³a &#9989;
  * Thiáº¿u kháº£ nÄƒng khÃ¡i quÃ¡t hoÃ¡: KhÃ´ng náº¯m báº¯t Ä‘Æ°á»£c Ã½ nghÄ©a ngá»¯ nghÄ©a tá»•ng quÃ¡t cá»§a vÄƒn báº£n mÃ  chá»‰ dá»±a vÃ o sá»± xuáº¥t hiá»‡n cá»§a tá»« khÃ³a. &#10060;

## 1.1 Sparse Embedding â˜•ï¸

***
### **ğŸ“˜ BM25 (Best Matching 25)**

BM25 lÃ  má»™t phÆ°Æ¡ng phÃ¡p xáº¿p háº¡ng tÃ i liá»‡u dá»±a trÃªn nguyÃªn táº¯c cá»§a TF-IDF nhÆ°ng Ä‘Æ°á»£c cáº£i tiáº¿n báº±ng cÃ¡ch chuáº©n hÃ³a Ä‘á»™ dÃ i cá»§a tÃ i liá»‡u. Äiá»u nÃ y giÃºp giáº£m thiá»ƒu sá»± thiÃªn lá»‡ch Ä‘á»‘i vá»›i cÃ¡c tÃ i liá»‡u cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau.

#### CÃ´ng thá»©c 

$$
\text{score}(D, Q) = \sum_{q \in Q} \text{IDF}(q) \cdot \frac{TF(q, D) \cdot (k_1 + 1)}{TF(q, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

#### ThÃ nh pháº§n

- **TF (Term Frequency):**  
  Táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« $q$ trong tÃ i liá»‡u $D$. Táº§n suáº¥t cao thÆ°á»ng cho tháº¥y tá»« Ä‘Ã³ cÃ³ Ã½ nghÄ©a quan trá»ng trong ná»™i dung tÃ i liá»‡u.

- **IDF (Inverse Document Frequency):**  
  Äá»™ hiáº¿m cá»§a tá»« $q$ trong toÃ n bá»™ kho tÃ i liá»‡u, giÃºp giáº£m trá»ng sá»‘ cho cÃ¡c tá»« quÃ¡ phá»• biáº¿n. GiÃºp giáº£m trá»ng sá»‘ cá»§a cÃ¡c tá»« xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn trÃªn toÃ n bá»™ táº­p tÃ i liá»‡u, nháº¥n máº¡nh cÃ¡c tá»« cÃ³ Ã½ nghÄ©a phÃ¢n biá»‡t cao.

- **$k_1$ vÃ  $b$:**  
  CÃ¡c tham sá»‘ Ä‘iá»u chá»‰nh (thÆ°á»ng máº·c Ä‘á»‹nh $k_1 \approx 1.2$ vÃ  $b \approx 0.75$):
  - $k_1$ Ä‘iá»u chá»‰nh áº£nh hÆ°á»Ÿng cá»§a táº§n suáº¥t tá»«.
  - $b$ Ä‘iá»u chá»‰nh áº£nh hÆ°á»Ÿng cá»§a Ä‘á»™ dÃ i tÃ i liá»‡u.

- **$|D|$ vÃ  $\text{avgdl}$:**  
  - $|D|$: Äá»™ dÃ i cá»§a tÃ i liá»‡u $D$ (sá»‘ tá»« trong tÃ i liá»‡u).
  - $\text{avgdl}$: Äá»™ dÃ i trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c tÃ i liá»‡u.

***
### **ğŸ“— SPLADE (Sparse Lexical and Expansion Model)**

#### CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a SPLADE

SPLADE (Sparse Lexical and Expansion model) lÃ  má»™t mÃ´ hÃ¬nh káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a cÃ¡c biá»ƒu diá»…n sparse (rá»i ráº¡c) vÃ  kháº£ nÄƒng má»Ÿ rá»™ng tá»« vá»±ng thÃ´ng qua há»c sÃ¢u. Má»¥c tiÃªu chÃ­nh cá»§a SPLADE lÃ  giáº£i quyáº¿t váº¥n Ä‘á» vocabulary mismatch trong truy váº¥n thÃ´ng tin báº±ng cÃ¡ch khÃ´ng chá»‰ dá»±a vÃ o cÃ¡

##### a. Khá»Ÿi Ä‘áº§u vá»›i Transformer vÃ  BERT
- **BERT vá»›i Masked Language Modeling (MLM):**  
  SPLADE sá»­ dá»¥ng mÃ´ hÃ¬nh BERT Ä‘Ã£ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n vá»›i nhiá»‡m vá»¥ MLM.
- **Tokenization & Embedding:**  
  VÄƒn báº£n Ä‘Æ°á»£c chia thÃ nh cÃ¡c token (hoáº·c sub-word tokens) theo kiá»ƒu BERT, má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh vector thÃ´ng qua embedding matrix.
  ![tokenizer](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fd773f2c0a10dc37381b4688626e4fdb9da5fc5a4-2310x1457.png&w=3840&q=75)
- **Encoder Blocks:**  
  CÃ¡c vector token Ä‘Æ°á»£c xá»­ lÃ½ qua nhiá»u lá»›p encoder vá»›i cÆ¡ cháº¿ attention, thu Ä‘Æ°á»£c cÃ¡c vector "information-rich" chá»©a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh.
  ![encoder-block](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe8fe02e5887ff8dda56dff29c18940b0125ebc6b-2318x1466.png&w=3840&q=75)

  ![output](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F00a694f2f4e9f7ad6006f538df551c5ec3c23347-2458x1363.png&w=3840&q=75)

##### b. Vai trÃ² cá»§a MLM Head
- **MLM Head:**  
  Má»™t sá»‘ token trong vÄƒn báº£n Ä‘Æ°á»£c thay báº±ng `[MASK]` vÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n láº¡i token gá»‘c dá»±a trÃªn ngá»¯ cáº£nh.
- **Output Logits:**  
  Káº¿t quáº£ Ä‘áº§u ra lÃ  táº­p há»£p logits cho má»—i token, vá»›i sá»‘ chiá»u báº±ng kÃ­ch thÆ°á»›c tá»« vá»±ng (vÃ­ dá»¥: 30522), biá»ƒu diá»…n xÃ¡c suáº¥t cá»§a cÃ¡c tá»« cÃ³ thá»ƒ xuáº¥t hiá»‡n táº¡i vá»‹ trÃ­ Ä‘Ã³.
  ![mask](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fd64d431fb1b50ae9aa94b5cd85e1cdffe5eb7ca1-2318x1516.png&w=3840&q=75)

##### c. Táº¡o Sparse Embeddings
- **Chuyá»ƒn Ä‘á»•i logits thÃ nh sparse vector:**  
  SPLADE tÃ­nh toÃ¡n trá»ng sá»‘ cho má»—i tá»« \(j\) trong tá»« vá»±ng theo cÃ´ng thá»©c:
  
  $$w_j = \sum_{i \in t} \log\Big(1 + \text{ReLU}(w_{ij})\Big)$$
  
  Trong Ä‘Ã³:
  - $t$: Táº­p cÃ¡c token cá»§a vÄƒn báº£n.
  - $w_{ij}$: Logit Ä‘Æ°á»£c dá»± Ä‘oÃ¡n cho token $i$ Ä‘á»‘i vá»›i tá»« $j$.
  
- **Má»Ÿ rá»™ng tá»« khÃ³a (Term Expansion):**  
  SPLADE khÃ´ng chá»‰ biá»ƒu diá»…n cÃ¡c tá»« cÃ³ trong vÄƒn báº£n mÃ  cÃ²n gÃ¡n trá»ng sá»‘ cho cÃ¡c tá»« liÃªn quan (vÃ­ dá»¥: tá»« "rainforest" cÃ³ thá»ƒ má»Ÿ rá»™ng thÃ nh "jungle", "land", "forest"), giÃºp tÄƒng kháº£ nÄƒng khá»›p giá»¯a truy váº¥n vÃ  tÃ i liá»‡u.

***
## 2. Multi-vector  ğŸ—¿

Matrices of numbers with fixed length but variable height. Usually obtained from late interaction models like ColBERT.

### ğŸ§ğŸ¼â€â™€ï¸ Colbert (Contextualized Late Interaction over BERT) 

KhÃ´ng giá»‘ng nhÆ° BERT, vá»‘n gá»™p cÃ¡c vector cá»§a tá»«ng token thÃ nh má»™t biá»ƒu diá»…n duy nháº¥t, ColBERT giá»¯ nguyÃªn cÃ¡c biá»ƒu diá»…n theo tá»«ng token, giÃºp tÃ­nh toÃ¡n má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng má»™t cÃ¡ch chi tiáº¿t hÆ¡n. Äiá»ƒm ná»•i báº­t cá»§a ColBERT chÃ­nh lÃ  cÆ¡ cháº¿ tÆ°Æ¡ng tÃ¡c muá»™n (late interaction). CÆ¡ cháº¿ nÃ y cho phÃ©p xáº¿p háº¡ng vÃ  truy xuáº¥t thÃ´ng tin má»™t cÃ¡ch hiá»‡u quáº£ vÃ  chÃ­nh xÃ¡c báº±ng cÃ¡ch xá»­ lÃ½ truy váº¥n vÃ  tÃ i liá»‡u má»™t cÃ¡ch Ä‘á»™c láº­p cho Ä‘áº¿n nhá»¯ng giai Ä‘oáº¡n cuá»‘i cá»§a quÃ¡ trÃ¬nh truy xuáº¥t.

**ColBERT Architecture** ğŸ›ï¸
![ colbert-architecture](https://assets.zilliz.com/The_general_architecture_of_Col_BERT_30db3739a3.png)

## 2.1 CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a ColBERT

###  Query Encoder

- **Tokenization**: Truy váº¥n $Q$Ä‘Æ°á»£c chia thÃ nh cÃ¡c token $q_1, q_2, \dots, q_l$.
- **ÄÃ¡nh dáº¥u**: ThÃªm token Ä‘áº·c biá»‡t `[Q]` ngay sau `[CLS]` Ä‘á»ƒ Ä‘Ã¡nh dáº¥u Ä‘Ã¢y lÃ  truy váº¥n.
- **Padding/Truncation**: Náº¿u sá»‘ token Ã­t hÆ¡n $N_q$, Ä‘á»‡m thÃªm token `[MASK]`; náº¿u nhiá»u hÆ¡n, cáº¯t ngáº¯n vá» $N_q$ token Ä‘áº§u.
- **Xá»­ lÃ½**: Chuá»—i token Ä‘Æ°á»£c Ä‘Æ°a qua BERT vÃ  sau Ä‘Ã³ qua má»™t CNN Ä‘á»ƒ táº¡o ra táº­p há»£p vector nhÃºng, rá»“i Ä‘Æ°á»£c chuáº©n hÃ³a:
  
  $$E_q := \text{Normalize}\Big(\text{CNN}\big(\text{BERT}("[Q], q_0, q_1, \dots, q_l, [MASK], \dots, [MASK]")\big)\Big)$$

---

### Document Encoder

- **Tokenization**: TÃ i liá»‡u $D$ Ä‘Æ°á»£c chia thÃ nh cÃ¡c token \( d_0, d_1, \dots, d_n \).
- **ÄÃ¡nh dáº¥u**: ThÃªm token Ä‘áº·c biá»‡t `[D]` ngay sau `[CLS]` Ä‘á»ƒ Ä‘Ã¡nh dáº¥u Ä‘áº§u tÃ i liá»‡u.
- **Xá»­ lÃ½**: Chuá»—i token Ä‘Æ°á»£c Ä‘Æ°a qua BERT, qua CNN, sau Ä‘Ã³ chuáº©n hÃ³a vÃ  lá»c bá» cÃ¡c vector liÃªn quan Ä‘áº¿n dáº¥u cÃ¢u:
  
$$E_d := \text{Filter}\Big(\text{Normalize}\big(\text{CNN}\big(\text{BERT}("[D], d_0, d_1, \dots, d_n")\big)\big)\Big)$$

---

### CÆ¡ cháº¿ tÆ°Æ¡ng tÃ¡c trá»… (Late Interaction Mechanism)
TtÆ°Æ¡ng tÃ¡c trá»… trong ColBERT nghÄ©a lÃ  truy váº¥n vÃ  tÃ i liá»‡u Ä‘Æ°á»£c mÃ£ hÃ³a riÃªng biá»‡t, chá»‰ so sÃ¡nh vá»›i nhau á»Ÿ bÆ°á»›c cuá»‘i cÃ¹ng. Äiá»u nÃ y giÃºp:

- TÄƒng tá»‘c Ä‘á»™ truy xuáº¥t tÃ i liá»‡u: CÃ³ thá»ƒ tÃ­nh trÆ°á»›c vector tÃ i liá»‡u vÃ  lÆ°u trá»¯ trÆ°á»›c khi cÃ³ truy váº¥n.
- Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c: So sÃ¡nh chi tiáº¿t tá»«ng token thay vÃ¬ má»™t vector duy nháº¥t.

- **MaxSim**:  
  Vá»›i má»—i token $t_q$ trong táº­p $E_q$:
  1. TÃ¬m token $t_d$ trong $E_d$ cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t (sá»­ dá»¥ng cosine similarity hoáº·c squared L2 distance).
  2. Tá»•ng há»£p cÃ¡c Ä‘iá»ƒm sá»‘ tÆ°Æ¡ng Ä‘á»“ng Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm liÃªn quan tá»•ng thá»ƒ $S_{q,d}$.

## 2.2 ColBERTv2 

- **Váº¥n Ä‘á» cá»§a ColBERT**: LÆ°u trá»¯ vector cho tá»«ng token tiÃªu tá»‘n nhiá»u bá»™ nhá»›.
- **Giáº£i phÃ¡p trong ColBERTv2**:
  - **Product Quantization (PQ)**: NÃ©n vector token mÃ  khÃ´ng máº¥t nhiá»u thÃ´ng tin.
  - **Centroid-Based Encoding**: NhÃ³m cÃ¡c vector theo trá»ng tÃ¢m Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c lÆ°u trá»¯.
- **Quy trÃ¬nh truy váº¥n**:
  - So sÃ¡nh chá»‰ vá»›i cÃ¡c vector thuá»™c cÃ¡c cá»¥m (centroid) gáº§n nháº¥t (sá»­ dá»¥ng sá»‘ lÆ°á»£ng cá»¥m Ä‘á»‹nh trÆ°á»›c, hay nprobe).
  - Sau Ä‘Ã³, táº£i láº¡i toÃ n bá»™ vector cá»§a tÃ i liá»‡u phÃ¹ há»£p Ä‘á»ƒ tÃ­nh toÃ¡n chi tiáº¿t hÆ¡n.

> **PhÃ¢n tÃ­ch**:  
> - ColBERTv2 giáº£m Ä‘Ã¡ng ká»ƒ dung lÆ°á»£ng lÆ°u trá»¯ trong khi váº«n giá»¯ Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao.
> - So khá»›p vá»›i cÃ¡c vector thuá»™c cá»¥m gáº§n nháº¥t giÃºp tÄƒng tá»‘c Ä‘á»™ truy xuáº¥t.

---